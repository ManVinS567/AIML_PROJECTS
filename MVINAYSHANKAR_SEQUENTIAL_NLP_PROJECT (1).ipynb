{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad709aa7",
   "metadata": {},
   "source": [
    "# PROJECT - SEQUENTIAL NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf44e0d",
   "metadata": {},
   "source": [
    "## PART-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e27437",
   "metadata": {},
   "source": [
    "- **DOMAIN:** Digital content and entertainment industry<br>\n",
    "<br>\n",
    "- **CONTEXT:** The objective of this project is to build a text classification model that analyses the customer's sentiments based on their reviews in the IMDB database. The model uses a complex deep learning model to build an embedding layer followed by a classification algorithm to analyse the sentiment of the customers.<br>\n",
    "<br>\n",
    "- **DATA DESCRIPTION:** The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocabulary size of 10,000. As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.<br>\n",
    "<br>\n",
    "- **PROJECT OBJECTIVE:** Build a sequential NLP classifier which can use input text parameters to determine the customer sentiments.\n",
    "<br>\n",
    "Steps and tasks:\n",
    "<br>\n",
    "1. Import and analyse the data set.<br>\n",
    "Hint: - Use `imdb.load_data()` method\n",
    "- Get train and test set\n",
    "- Take 10000 most frequent words\n",
    "<br>\n",
    "2. Perform relevant sequence adding on the data<br>\n",
    "<br>\n",
    "3. Perform following data analysis:<br>\n",
    "• Print shape of features and labels<br>\n",
    "• Print value of any one feature and it's label<br>\n",
    "<br>\n",
    "4. Decode the feature value to get original sentence<br>\n",
    "<br>\n",
    "5. Design, train, tune and test a sequential model.<br>\n",
    "<br>\n",
    "Hint: The aim here Is to import the text, process it such a way that it can be taken as an inout to the ML/NN\n",
    "classifiers. Be analytical and experimental here in trying new approaches to design the best model.\n",
    "<br><br>\n",
    "6. Use the designed model to print the prediction on any one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29559d14",
   "metadata": {},
   "source": [
    "### 1. Import and analyse the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4af099",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the libraries and dataset\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPool1D, LSTM, TimeDistributed, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb2100",
   "metadata": {},
   "source": [
    "#### - Get the train and test set\n",
    "#### - Take 10000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b78539",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000, maxlen=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aed03d",
   "metadata": {},
   "source": [
    "##### Know the shape of the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51ae05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19051,)\n",
      "(19051,)\n",
      "[1 0 0 ... 0 1 0]\n",
      "[0 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49952394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels : \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of labels\n",
    "print(\"Labels : \")\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0ce833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of sequence in X_train: 299\n",
      "Max length of sequence in X_test: 299\n"
     ]
    }
   ],
   "source": [
    "X_train_max_len = max(len(x) for x in X_train)\n",
    "print(f\"Max length of sequence in X_train: {X_train_max_len}\")\n",
    "\n",
    "X_test_max_len = max(len(x) for x in X_test)\n",
    "print(f\"Max length of sequence in X_test: {X_test_max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339935b7",
   "metadata": {},
   "source": [
    "#### 2. Perform relevant sequence adding on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae448b5",
   "metadata": {},
   "source": [
    "#### Padding each sentence to be of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bf8140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 159.68 words (60.730229)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X_train]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "#plot review length\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecc1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding to 300 length\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=300)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cceedc6",
   "metadata": {},
   "source": [
    "#### 3. Perform following data analysis:\n",
    "- Print shape of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e466be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in X_train: 19051\n",
      "Number of reviews in X_test: 19450\n",
      "Number of words in each review is 300\n"
     ]
    }
   ],
   "source": [
    "# Number of reviews, number of words in each review\n",
    "# Number of reviews\n",
    "print(f\"Number of reviews in X_train: {len(X_train)}\")\n",
    "print(f\"Number of reviews in X_test: {len(X_test)}\")\n",
    "\n",
    "# number of words in each review\n",
    "print(f\"Number of words in each review is {X_train_padded[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014185b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19051,)\n",
      "(19450,)\n"
     ]
    }
   ],
   "source": [
    "#Number of labels\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673c1cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "#print unique lables \n",
    "print(f\"Unique labels: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee38b0a",
   "metadata": {},
   "source": [
    "- Print value of any one feature and it's label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6457558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the 100th review: [1, 14, 9, 6, 389, 20, 19, 6, 253, 1096, 65, 5, 4, 6891, 7, 1181, 3855, 5, 4, 620, 756, 7, 803, 674, 11, 113, 97, 14, 6, 55, 467, 2525, 20, 92, 387, 4, 2, 7, 4, 182, 2, 25, 39, 319, 14, 2065, 47, 389, 388, 5, 13, 594, 33, 4, 192, 15, 212, 9, 115, 2525, 88, 156, 40, 2065, 97, 170, 39, 486, 8, 622, 1801, 168, 6529, 776, 87, 20, 32, 187]\n"
     ]
    }
   ],
   "source": [
    "#Feature value\n",
    "print(f\"Value of the 100th review: {X_train[100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3000fc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Label : 1\n"
     ]
    }
   ],
   "source": [
    "#Label value\n",
    "print(f\"Sentiment Label : {y_train[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db7d09",
   "metadata": {},
   "source": [
    "#### Decode the feature value to get original sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e50a9",
   "metadata": {},
   "source": [
    "Retrieve a dictionary that contains mapping of words to their index in the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece6b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fea95",
   "metadata": {},
   "source": [
    "Now use the dictionary to get the original words from the encodings for a particular sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f58d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_from = 3\n",
    "imdb_word_index = {key:value + index_from for (key,value) in imdb_word_index.items()}\n",
    "imdb_word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fdd664d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'movie',\n",
       " 'with',\n",
       " 'a',\n",
       " 'fun',\n",
       " 'clever',\n",
       " 'story',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dynamics',\n",
       " 'of',\n",
       " 'culture',\n",
       " 'differences',\n",
       " 'and',\n",
       " 'the',\n",
       " 'running',\n",
       " 'theme',\n",
       " 'of',\n",
       " \"what's\",\n",
       " 'important',\n",
       " 'in',\n",
       " 'life',\n",
       " 'make',\n",
       " 'this',\n",
       " 'a',\n",
       " 'very',\n",
       " 'under',\n",
       " 'appreciated',\n",
       " 'movie',\n",
       " \"don't\",\n",
       " 'let',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'you',\n",
       " 'from',\n",
       " 'seeing',\n",
       " 'this',\n",
       " 'keaton',\n",
       " 'has',\n",
       " 'wonderful',\n",
       " 'moments',\n",
       " 'and',\n",
       " 'i',\n",
       " 'wonder',\n",
       " 'at',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'comedy',\n",
       " 'is',\n",
       " 'never',\n",
       " 'appreciated',\n",
       " 'because',\n",
       " 'actors',\n",
       " 'like',\n",
       " 'keaton',\n",
       " 'make',\n",
       " 'going',\n",
       " 'from',\n",
       " 'humor',\n",
       " 'to',\n",
       " 'serious',\n",
       " 'bits',\n",
       " 'look',\n",
       " 'tremendously',\n",
       " 'easy',\n",
       " 'great',\n",
       " 'movie',\n",
       " 'all',\n",
       " 'around']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index = {value:key for key,value in imdb_word_index.items()}\n",
    "[reverse_word_index[n] for n in X_train[100] if n > index_from]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d009bb3",
   "metadata": {},
   "source": [
    "Get the sentiment for the above sentence\n",
    "- postive(1)\n",
    "- negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25d5769e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8cc24",
   "metadata": {},
   "source": [
    "#### 5. Design, train, tune and test a sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88038ebe",
   "metadata": {},
   "source": [
    "##### Define model\n",
    "- Defining a sequential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75fd8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 10000\n",
    "max_words = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, 100, input_length = max_words))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "dense_layer = Dense(100, activation='relu')\n",
    "model.add(TimeDistributed(dense_layer))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02584b7",
   "metadata": {},
   "source": [
    "##### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf717e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c72791",
   "metadata": {},
   "source": [
    "##### Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60573cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 300, 100)          80400     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 300, 100)         10100     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 30001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,120,501\n",
      "Trainable params: 1,120,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90078172",
   "metadata": {},
   "source": [
    "##### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e57d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "298/298 [==============================] - 262s 866ms/step - loss: 0.3750 - accuracy: 0.8214\n",
      "Epoch 2/5\n",
      "298/298 [==============================] - 268s 900ms/step - loss: 0.1788 - accuracy: 0.9319\n",
      "Epoch 3/5\n",
      "298/298 [==============================] - 271s 910ms/step - loss: 0.1060 - accuracy: 0.9617\n",
      "Epoch 4/5\n",
      "298/298 [==============================] - 269s 903ms/step - loss: 0.0602 - accuracy: 0.9783\n",
      "Epoch 5/5\n",
      "298/298 [==============================] - 274s 919ms/step - loss: 0.0393 - accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22bd264ceb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5e68e",
   "metadata": {},
   "source": [
    "##### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2ec265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6400\n",
      "Accuracy : 0.8645\n"
     ]
    }
   ],
   "source": [
    "scores,accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "print(\"Score: {:.4f}\".format(scores))\n",
    "print(\"Accuracy : {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80c650",
   "metadata": {},
   "source": [
    "##### 6. Use the designed model to print the prediction on any one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d64ab7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicting using a simple review\n",
    "\n",
    "good_review = \"i liked the movie\"\n",
    "bad_review = \"i did not like this movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d54f686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_word_index['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81e6011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: i liked the movie\n",
      "\tSentiment: positive\n",
      "Review: i did not like this movie\n",
      "\tSentiment: negative\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "for review in [good_review, bad_review]:\n",
    "    encoded_review = []\n",
    "    review_split = review.split(\" \")\n",
    "    for word in review_split:\n",
    "        encoded_review.append(imdb_word_index[word])\n",
    "    review_padded = pad_sequences([encoded_review], maxlen=300)\n",
    "    pred = model.predict(review_padded)\n",
    "    if pred > 0.5:\n",
    "        sentiment = 'positive'\n",
    "    else:\n",
    "        sentiment = 'negative'\n",
    "    print(\"Review: {0}\\n\\tSentiment: {1}\".format(review,sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa6317",
   "metadata": {},
   "source": [
    "## PART-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67272024",
   "metadata": {},
   "source": [
    "- **DOMAIN:** Social media analytics<BR>\n",
    "    <br>\n",
    "- **CONTEXT:** Past studies in Sarcasm Detection mostly make use of Twitter datasets collected\n",
    "using hashtag based supervision but such datasets are noisy in terms of labels and\n",
    "language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in\n",
    "these requires the availability of contextual tweets.In this hands-on project, the goal is to\n",
    "build a model to detect whether a sentence is sarcastic or not, using Bidirectional LSTMs.<BR>\n",
    "    <br>\n",
    "- **DATA DESCRIPTION:** The dataset is collected from two news websites, theonion.com and huffingtonpost.com. \n",
    "    This new dataset has the following advantages over the existing Twitter datasets:\n",
    "Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings. Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets. Unlike tweets that reply to other tweets, the news headlines obtained are self-contained. This would help us in\n",
    "teasing apart the real sarcastic elements\n",
    "Content: Each record consists of three attributes:\n",
    "is_sarcastic: 1 if the record is sarcastic otherwise 0\n",
    "headline: the headline of the news article\n",
    "article_link: link to the original news article. Useful in collecting supplementary data\n",
    "Reference: https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection<BR>\n",
    "    <br>\n",
    "- **PROJECT OBJECTIVE:** Build a sequential NLP classifier which can use input text parameters to determine the customer sentiments.<BR>\n",
    "    <br>\n",
    "Steps and tasks: <BR>\n",
    "    <br>\n",
    "1. Read and explore the data\n",
    "2. Retain relevant columns\n",
    "3. Get length of each sentence\n",
    "4. Define parameters\n",
    "5. Get indices for words\n",
    "6. Create features and labels\n",
    "7. Get vocabulary size\n",
    "8. Create a weight matrix using GloVe embeddings\n",
    "9. Define and compile a Bidirectional LSTM model.\n",
    "Hint: Be analytical and experimental here in trying new approaches to design the best model.\n",
    "10. Fit the model and check the validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513b5cb",
   "metadata": {},
   "source": [
    "## Sarcasm Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb1eee",
   "metadata": {},
   "source": [
    "#### 1. Read and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce174a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35837ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/american-...</td>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/americas-...</td>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/reparatio...</td>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/israeli-b...</td>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/gourmet-g...</td>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "0      https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1      https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2      https://local.theonion.com/mom-starting-to-fea...   \n",
       "3      https://politics.theonion.com/boehner-just-wan...   \n",
       "4      https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "...                                                  ...   \n",
       "26704  https://www.huffingtonpost.com/entry/american-...   \n",
       "26705  https://www.huffingtonpost.com/entry/americas-...   \n",
       "26706  https://www.huffingtonpost.com/entry/reparatio...   \n",
       "26707  https://www.huffingtonpost.com/entry/israeli-b...   \n",
       "26708  https://www.huffingtonpost.com/entry/gourmet-g...   \n",
       "\n",
       "                                                headline  is_sarcastic  \n",
       "0      former versace store clerk sues over secret 'b...             0  \n",
       "1      the 'roseanne' revival catches up to our thorn...             0  \n",
       "2      mom starting to fear son's web series closest ...             1  \n",
       "3      boehner just wants wife to listen, not come up...             1  \n",
       "4      j.k. rowling wishes snape happy birthday in th...             0  \n",
       "...                                                  ...           ...  \n",
       "26704               american politics in moral free-fall             0  \n",
       "26705                            america's best 20 hikes             0  \n",
       "26706                              reparations and obama             0  \n",
       "26707  israeli ban targeting boycott supporters raise...             0  \n",
       "26708                  gourmet gifts for the foodie 2014             0  \n",
       "\n",
       "[26709 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# know the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7ff4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26709, 3)\n"
     ]
    }
   ],
   "source": [
    "# to check the shape of the data\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4239ae0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_link', 'headline', 'is_sarcastic'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data contains 3 columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdaa152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the 'roseanne' revival catches up to our thorny political mood, for better and worse\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073f6f3",
   "metadata": {},
   "source": [
    "#### 2. Retain relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff8b4e",
   "metadata": {},
   "source": [
    "##### Dropping article_link from dataset, since it is not significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aee7e9",
   "metadata": {},
   "source": [
    "As we need only headline text data and is_sarcastic column for the current project. We are dropping article_link column here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8269fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('article_link', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c11e8a",
   "metadata": {},
   "source": [
    "#### 3. Get the length of each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2affae",
   "metadata": {},
   "source": [
    "As different lines are of different length. We need to pad our sequences using the max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d6d7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(text) for text in data['headline']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f688ae7",
   "metadata": {},
   "source": [
    "#####  Import required modules for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b0ef440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af19b3",
   "metadata": {},
   "source": [
    "#### 4. Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "111a3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = max([len(text) for text in data['headline']])\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcc003",
   "metadata": {},
   "source": [
    "#### 5.Get indices for words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362cf089",
   "metadata": {},
   "source": [
    "## Apply Keras Tokenizer of headline column of your data.\n",
    "Hint - First create a tokenizer instance using Tokenizer(num_words=max_features) \n",
    "And then fit this tokenizer instance on your data column df['headline'] using .fit_on_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts(data['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "664af7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1,\n",
       " 'of': 2,\n",
       " 'the': 3,\n",
       " 'in': 4,\n",
       " 'for': 5,\n",
       " 'a': 6,\n",
       " 'on': 7,\n",
       " 'and': 8,\n",
       " 'with': 9,\n",
       " 'is': 10,\n",
       " 'new': 11,\n",
       " 'trump': 12,\n",
       " 'man': 13,\n",
       " 'from': 14,\n",
       " 'at': 15,\n",
       " 'about': 16,\n",
       " 'you': 17,\n",
       " 'this': 18,\n",
       " 'by': 19,\n",
       " 'after': 20,\n",
       " 'up': 21,\n",
       " 'out': 22,\n",
       " 'be': 23,\n",
       " 'how': 24,\n",
       " 'as': 25,\n",
       " 'it': 26,\n",
       " 'that': 27,\n",
       " 'not': 28,\n",
       " 'are': 29,\n",
       " 'your': 30,\n",
       " 'his': 31,\n",
       " 'what': 32,\n",
       " 'he': 33,\n",
       " 'all': 34,\n",
       " 'just': 35,\n",
       " 'who': 36,\n",
       " 'has': 37,\n",
       " 'will': 38,\n",
       " 'more': 39,\n",
       " 'one': 40,\n",
       " 'into': 41,\n",
       " 'report': 42,\n",
       " 'year': 43,\n",
       " 'why': 44,\n",
       " 'have': 45,\n",
       " 'area': 46,\n",
       " 'over': 47,\n",
       " 'donald': 48,\n",
       " 'u': 49,\n",
       " 'day': 50,\n",
       " 'says': 51,\n",
       " 's': 52,\n",
       " 'can': 53,\n",
       " 'first': 54,\n",
       " 'woman': 55,\n",
       " 'time': 56,\n",
       " 'like': 57,\n",
       " 'her': 58,\n",
       " \"trump's\": 59,\n",
       " 'old': 60,\n",
       " 'no': 61,\n",
       " 'get': 62,\n",
       " 'off': 63,\n",
       " 'an': 64,\n",
       " 'life': 65,\n",
       " 'people': 66,\n",
       " 'obama': 67,\n",
       " 'now': 68,\n",
       " 'house': 69,\n",
       " 'still': 70,\n",
       " \"'\": 71,\n",
       " 'women': 72,\n",
       " 'make': 73,\n",
       " 'was': 74,\n",
       " 'than': 75,\n",
       " 'white': 76,\n",
       " 'back': 77,\n",
       " 'my': 78,\n",
       " 'i': 79,\n",
       " 'clinton': 80,\n",
       " 'down': 81,\n",
       " 'if': 82,\n",
       " '5': 83,\n",
       " 'when': 84,\n",
       " 'world': 85,\n",
       " 'could': 86,\n",
       " 'we': 87,\n",
       " 'their': 88,\n",
       " 'before': 89,\n",
       " 'americans': 90,\n",
       " 'way': 91,\n",
       " 'do': 92,\n",
       " 'family': 93,\n",
       " 'most': 94,\n",
       " 'gop': 95,\n",
       " 'they': 96,\n",
       " 'study': 97,\n",
       " 'school': 98,\n",
       " \"it's\": 99,\n",
       " 'black': 100,\n",
       " 'best': 101,\n",
       " 'years': 102,\n",
       " 'bill': 103,\n",
       " 'should': 104,\n",
       " '3': 105,\n",
       " 'him': 106,\n",
       " 'would': 107,\n",
       " 'so': 108,\n",
       " 'police': 109,\n",
       " 'only': 110,\n",
       " 'watch': 111,\n",
       " 'american': 112,\n",
       " 'really': 113,\n",
       " 'being': 114,\n",
       " 'but': 115,\n",
       " 'last': 116,\n",
       " 'know': 117,\n",
       " '10': 118,\n",
       " \"can't\": 119,\n",
       " 'death': 120,\n",
       " 'home': 121,\n",
       " 'during': 122,\n",
       " 'video': 123,\n",
       " 'finds': 124,\n",
       " 'state': 125,\n",
       " 'or': 126,\n",
       " 'president': 127,\n",
       " 'health': 128,\n",
       " 'going': 129,\n",
       " 'say': 130,\n",
       " 'show': 131,\n",
       " 'nation': 132,\n",
       " 'good': 133,\n",
       " 'things': 134,\n",
       " 'hillary': 135,\n",
       " \"'the\": 136,\n",
       " 'may': 137,\n",
       " '2': 138,\n",
       " 'against': 139,\n",
       " 'campaign': 140,\n",
       " 'every': 141,\n",
       " 'she': 142,\n",
       " 'love': 143,\n",
       " 'mom': 144,\n",
       " 'need': 145,\n",
       " 'big': 146,\n",
       " 'right': 147,\n",
       " 'party': 148,\n",
       " 'gets': 149,\n",
       " '000': 150,\n",
       " 'too': 151,\n",
       " 'getting': 152,\n",
       " 'these': 153,\n",
       " 'kids': 154,\n",
       " 'some': 155,\n",
       " 'parents': 156,\n",
       " 'work': 157,\n",
       " 'court': 158,\n",
       " 'little': 159,\n",
       " 'change': 160,\n",
       " 'take': 161,\n",
       " 'high': 162,\n",
       " 'makes': 163,\n",
       " 'self': 164,\n",
       " 'our': 165,\n",
       " 'calls': 166,\n",
       " 'john': 167,\n",
       " 'other': 168,\n",
       " 'news': 169,\n",
       " 'through': 170,\n",
       " \"doesn't\": 171,\n",
       " 'while': 172,\n",
       " \"here's\": 173,\n",
       " 'never': 174,\n",
       " 'child': 175,\n",
       " 'gay': 176,\n",
       " 'dead': 177,\n",
       " 'look': 178,\n",
       " 'election': 179,\n",
       " 'want': 180,\n",
       " 'own': 181,\n",
       " '4': 182,\n",
       " \"don't\": 183,\n",
       " 'see': 184,\n",
       " 'takes': 185,\n",
       " 'america': 186,\n",
       " '7': 187,\n",
       " 'local': 188,\n",
       " 'real': 189,\n",
       " 'where': 190,\n",
       " 'next': 191,\n",
       " 'stop': 192,\n",
       " 'even': 193,\n",
       " 'its': 194,\n",
       " \"he's\": 195,\n",
       " 'war': 196,\n",
       " 'college': 197,\n",
       " 'go': 198,\n",
       " '6': 199,\n",
       " \"nation's\": 200,\n",
       " 'sex': 201,\n",
       " 'bush': 202,\n",
       " 'made': 203,\n",
       " 'plan': 204,\n",
       " 'office': 205,\n",
       " 'again': 206,\n",
       " 'guy': 207,\n",
       " 'two': 208,\n",
       " 'dad': 209,\n",
       " 'another': 210,\n",
       " 'around': 211,\n",
       " 'dog': 212,\n",
       " 'got': 213,\n",
       " '1': 214,\n",
       " 'million': 215,\n",
       " 'ever': 216,\n",
       " 'week': 217,\n",
       " 'baby': 218,\n",
       " 'debate': 219,\n",
       " 'thing': 220,\n",
       " 'them': 221,\n",
       " 'gun': 222,\n",
       " 'wants': 223,\n",
       " 'care': 224,\n",
       " 'us': 225,\n",
       " 'help': 226,\n",
       " 'much': 227,\n",
       " 'long': 228,\n",
       " 'night': 229,\n",
       " 'congress': 230,\n",
       " 'job': 231,\n",
       " 'finally': 232,\n",
       " 'north': 233,\n",
       " 'been': 234,\n",
       " 'under': 235,\n",
       " \"man's\": 236,\n",
       " 'actually': 237,\n",
       " 'star': 238,\n",
       " 'national': 239,\n",
       " 'live': 240,\n",
       " 'climate': 241,\n",
       " 'season': 242,\n",
       " 'money': 243,\n",
       " 'couple': 244,\n",
       " \"won't\": 245,\n",
       " '8': 246,\n",
       " '9': 247,\n",
       " 'top': 248,\n",
       " 'god': 249,\n",
       " 'anti': 250,\n",
       " 'media': 251,\n",
       " 'food': 252,\n",
       " 'ways': 253,\n",
       " '20': 254,\n",
       " 'shows': 255,\n",
       " 'sexual': 256,\n",
       " 'better': 257,\n",
       " 'give': 258,\n",
       " 'shooting': 259,\n",
       " 'had': 260,\n",
       " 'teen': 261,\n",
       " 'face': 262,\n",
       " 'making': 263,\n",
       " 'game': 264,\n",
       " 'paul': 265,\n",
       " 'reveals': 266,\n",
       " 'me': 267,\n",
       " 'trying': 268,\n",
       " 'senate': 269,\n",
       " 'supreme': 270,\n",
       " 'announces': 271,\n",
       " 'there': 272,\n",
       " 'away': 273,\n",
       " 'men': 274,\n",
       " 'history': 275,\n",
       " 'business': 276,\n",
       " 'bad': 277,\n",
       " 'without': 278,\n",
       " 'students': 279,\n",
       " 'everyone': 280,\n",
       " 'attack': 281,\n",
       " 'end': 282,\n",
       " 'story': 283,\n",
       " 'fight': 284,\n",
       " 'facebook': 285,\n",
       " 'son': 286,\n",
       " 'free': 287,\n",
       " 'children': 288,\n",
       " 'enough': 289,\n",
       " 'tv': 290,\n",
       " 'law': 291,\n",
       " 'movie': 292,\n",
       " 'city': 293,\n",
       " 'any': 294,\n",
       " 'introduces': 295,\n",
       " 'pope': 296,\n",
       " 'deal': 297,\n",
       " 'government': 298,\n",
       " 'body': 299,\n",
       " 'part': 300,\n",
       " 'york': 301,\n",
       " '11': 302,\n",
       " 'tell': 303,\n",
       " 'great': 304,\n",
       " 'film': 305,\n",
       " 'does': 306,\n",
       " 'former': 307,\n",
       " 'single': 308,\n",
       " 'entire': 309,\n",
       " 'friends': 310,\n",
       " 'fire': 311,\n",
       " 'call': 312,\n",
       " 'found': 313,\n",
       " 'friend': 314,\n",
       " 'book': 315,\n",
       " 'wedding': 316,\n",
       " 'think': 317,\n",
       " 'come': 318,\n",
       " 'republican': 319,\n",
       " 'must': 320,\n",
       " 'girl': 321,\n",
       " 'find': 322,\n",
       " 'second': 323,\n",
       " 'middle': 324,\n",
       " 'morning': 325,\n",
       " 'support': 326,\n",
       " 'same': 327,\n",
       " 'speech': 328,\n",
       " 'public': 329,\n",
       " 'photos': 330,\n",
       " 'use': 331,\n",
       " 'talk': 332,\n",
       " 'line': 333,\n",
       " 'car': 334,\n",
       " 'sanders': 335,\n",
       " 'name': 336,\n",
       " 'keep': 337,\n",
       " 'thinks': 338,\n",
       " 'run': 339,\n",
       " 'already': 340,\n",
       " 'looking': 341,\n",
       " 'presidential': 342,\n",
       " 'coming': 343,\n",
       " 'james': 344,\n",
       " 'republicans': 345,\n",
       " 'email': 346,\n",
       " \"didn't\": 347,\n",
       " 'tax': 348,\n",
       " 'pretty': 349,\n",
       " 'case': 350,\n",
       " 'company': 351,\n",
       " 'behind': 352,\n",
       " 'rights': 353,\n",
       " 'power': 354,\n",
       " 'open': 355,\n",
       " 'future': 356,\n",
       " 'marriage': 357,\n",
       " 'between': 358,\n",
       " 'releases': 359,\n",
       " 'violence': 360,\n",
       " 'christmas': 361,\n",
       " 'security': 362,\n",
       " '2016': 363,\n",
       " \"world's\": 364,\n",
       " 'used': 365,\n",
       " 'human': 366,\n",
       " 'killed': 367,\n",
       " 'voters': 368,\n",
       " 'once': 369,\n",
       " 'control': 370,\n",
       " 'goes': 371,\n",
       " 'group': 372,\n",
       " 'vote': 373,\n",
       " 'win': 374,\n",
       " 'might': 375,\n",
       " 'democrats': 376,\n",
       " 'student': 377,\n",
       " 'full': 378,\n",
       " 'something': 379,\n",
       " 'doing': 380,\n",
       " 'secret': 381,\n",
       " 'asks': 382,\n",
       " 'fans': 383,\n",
       " '12': 384,\n",
       " 'having': 385,\n",
       " 'team': 386,\n",
       " 'bernie': 387,\n",
       " 'department': 388,\n",
       " 'twitter': 389,\n",
       " 'room': 390,\n",
       " 'ban': 391,\n",
       " 'ad': 392,\n",
       " 'because': 393,\n",
       " 'poll': 394,\n",
       " 'teacher': 395,\n",
       " 'female': 396,\n",
       " 'post': 397,\n",
       " 'each': 398,\n",
       " 'wife': 399,\n",
       " 'inside': 400,\n",
       " 'ryan': 401,\n",
       " 'sure': 402,\n",
       " 'race': 403,\n",
       " 'claims': 404,\n",
       " 'music': 405,\n",
       " 'three': 406,\n",
       " 'meet': 407,\n",
       " 'record': 408,\n",
       " 'art': 409,\n",
       " 'forced': 410,\n",
       " 'boy': 411,\n",
       " '15': 412,\n",
       " 'missing': 413,\n",
       " 'many': 414,\n",
       " 'political': 415,\n",
       " 'unveils': 416,\n",
       " 'perfect': 417,\n",
       " 'head': 418,\n",
       " 'super': 419,\n",
       " 'very': 420,\n",
       " 'photo': 421,\n",
       " 'judge': 422,\n",
       " 'running': 423,\n",
       " 'reports': 424,\n",
       " 'red': 425,\n",
       " 'father': 426,\n",
       " 'save': 427,\n",
       " 'class': 428,\n",
       " 'scientists': 429,\n",
       " 'month': 430,\n",
       " 'plans': 431,\n",
       " 'days': 432,\n",
       " 'country': 433,\n",
       " 'person': 434,\n",
       " 'living': 435,\n",
       " 'tells': 436,\n",
       " 'social': 437,\n",
       " 'minutes': 438,\n",
       " 'put': 439,\n",
       " 'summer': 440,\n",
       " 'everything': 441,\n",
       " 'dies': 442,\n",
       " 'california': 443,\n",
       " 'always': 444,\n",
       " 'until': 445,\n",
       " 'obamacare': 446,\n",
       " 'states': 447,\n",
       " 'here': 448,\n",
       " 'pay': 449,\n",
       " 'ready': 450,\n",
       " 'texas': 451,\n",
       " 'were': 452,\n",
       " 'michael': 453,\n",
       " 'looks': 454,\n",
       " 'employee': 455,\n",
       " 'talks': 456,\n",
       " 'candidate': 457,\n",
       " 'needs': 458,\n",
       " 'did': 459,\n",
       " 'eating': 460,\n",
       " 'working': 461,\n",
       " 'water': 462,\n",
       " 'list': 463,\n",
       " 'justice': 464,\n",
       " 'secretary': 465,\n",
       " 'shot': 466,\n",
       " 'hot': 467,\n",
       " 'warns': 468,\n",
       " 'times': 469,\n",
       " 'comes': 470,\n",
       " 'past': 471,\n",
       " 'admits': 472,\n",
       " 'set': 473,\n",
       " 'start': 474,\n",
       " 'taking': 475,\n",
       " 'wall': 476,\n",
       " 'heart': 477,\n",
       " 'ceo': 478,\n",
       " 'ex': 479,\n",
       " 'thought': 480,\n",
       " \"'i\": 481,\n",
       " 'lives': 482,\n",
       " 'age': 483,\n",
       " 'left': 484,\n",
       " 'mike': 485,\n",
       " 'mother': 486,\n",
       " 'town': 487,\n",
       " 'gives': 488,\n",
       " '30': 489,\n",
       " 'let': 490,\n",
       " 'cruz': 491,\n",
       " \"women's\": 492,\n",
       " 'kim': 493,\n",
       " 'russia': 494,\n",
       " 'idea': 495,\n",
       " 'drug': 496,\n",
       " 'chief': 497,\n",
       " 'phone': 498,\n",
       " \"you're\": 499,\n",
       " 'cancer': 500,\n",
       " 'george': 501,\n",
       " 'crisis': 502,\n",
       " 'service': 503,\n",
       " 'biden': 504,\n",
       " 'wins': 505,\n",
       " 'hours': 506,\n",
       " \"i'm\": 507,\n",
       " 'letter': 508,\n",
       " 'wrong': 509,\n",
       " 'tips': 510,\n",
       " 'meeting': 511,\n",
       " 'south': 512,\n",
       " 'korea': 513,\n",
       " 'lost': 514,\n",
       " 'breaking': 515,\n",
       " 'daughter': 516,\n",
       " 'air': 517,\n",
       " '50': 518,\n",
       " 'probably': 519,\n",
       " 'young': 520,\n",
       " 'fbi': 521,\n",
       " 'street': 522,\n",
       " 'dream': 523,\n",
       " 'percent': 524,\n",
       " 'yet': 525,\n",
       " 'education': 526,\n",
       " 'isis': 527,\n",
       " 'romney': 528,\n",
       " 'word': 529,\n",
       " 'thousands': 530,\n",
       " 'restaurant': 531,\n",
       " 'small': 532,\n",
       " 'nuclear': 533,\n",
       " 'fucking': 534,\n",
       " 'kill': 535,\n",
       " 'today': 536,\n",
       " 'believe': 537,\n",
       " 'king': 538,\n",
       " 'tweets': 539,\n",
       " 'together': 540,\n",
       " 'half': 541,\n",
       " 'someone': 542,\n",
       " 'ted': 543,\n",
       " 'hard': 544,\n",
       " 'questions': 545,\n",
       " 'military': 546,\n",
       " 'march': 547,\n",
       " \"she's\": 548,\n",
       " 'few': 549,\n",
       " 'administration': 550,\n",
       " 'owner': 551,\n",
       " 'feel': 552,\n",
       " 'cat': 553,\n",
       " 'leaves': 554,\n",
       " 'fan': 555,\n",
       " 'internet': 556,\n",
       " 'officials': 557,\n",
       " 'third': 558,\n",
       " 'talking': 559,\n",
       " 'nothing': 560,\n",
       " 'director': 561,\n",
       " 'federal': 562,\n",
       " 'sleep': 563,\n",
       " 'chris': 564,\n",
       " 'rock': 565,\n",
       " 'place': 566,\n",
       " \"what's\": 567,\n",
       " 'washington': 568,\n",
       " 'guide': 569,\n",
       " 'online': 570,\n",
       " 'attacks': 571,\n",
       " 'muslim': 572,\n",
       " 'earth': 573,\n",
       " 'giving': 574,\n",
       " 'move': 575,\n",
       " 'lot': 576,\n",
       " 'florida': 577,\n",
       " 'ask': 578,\n",
       " 'iran': 579,\n",
       " 'latest': 580,\n",
       " 'series': 581,\n",
       " 'holiday': 582,\n",
       " 'congressman': 583,\n",
       " 'community': 584,\n",
       " 'abortion': 585,\n",
       " 'well': 586,\n",
       " 'order': 587,\n",
       " 'buy': 588,\n",
       " 'personal': 589,\n",
       " 'less': 590,\n",
       " 'months': 591,\n",
       " 'majority': 592,\n",
       " 'birthday': 593,\n",
       " 'hour': 594,\n",
       " 't': 595,\n",
       " 'prison': 596,\n",
       " '2015': 597,\n",
       " 'democratic': 598,\n",
       " 'outside': 599,\n",
       " 'problem': 600,\n",
       " 'leave': 601,\n",
       " 'assault': 602,\n",
       " 'those': 603,\n",
       " 'shit': 604,\n",
       " 'travel': 605,\n",
       " 'hollywood': 606,\n",
       " 'wearing': 607,\n",
       " 'beautiful': 608,\n",
       " 'girlfriend': 609,\n",
       " \"isn't\": 610,\n",
       " 'ice': 611,\n",
       " 'reason': 612,\n",
       " 'bar': 613,\n",
       " 'francis': 614,\n",
       " 'told': 615,\n",
       " 'different': 616,\n",
       " 'favorite': 617,\n",
       " 'issues': 618,\n",
       " 'cover': 619,\n",
       " 'rules': 620,\n",
       " 'rise': 621,\n",
       " 'happy': 622,\n",
       " 'fox': 623,\n",
       " 'fun': 624,\n",
       " 'special': 625,\n",
       " 'mark': 626,\n",
       " 'system': 627,\n",
       " 'read': 628,\n",
       " 'watching': 629,\n",
       " 'reasons': 630,\n",
       " 'girls': 631,\n",
       " 'straight': 632,\n",
       " 'play': 633,\n",
       " \"america's\": 634,\n",
       " 'al': 635,\n",
       " 'celebrates': 636,\n",
       " \"obama's\": 637,\n",
       " 'minute': 638,\n",
       " 'thinking': 639,\n",
       " 'hate': 640,\n",
       " 'excited': 641,\n",
       " 'relationship': 642,\n",
       " 'trip': 643,\n",
       " 'hit': 644,\n",
       " 'response': 645,\n",
       " 'huffpost': 646,\n",
       " 'knows': 647,\n",
       " 'russian': 648,\n",
       " 'immigration': 649,\n",
       " 'protest': 650,\n",
       " 'scott': 651,\n",
       " 'following': 652,\n",
       " '100': 653,\n",
       " 'using': 654,\n",
       " 'offers': 655,\n",
       " 'front': 656,\n",
       " 'message': 657,\n",
       " 'trailer': 658,\n",
       " 'stars': 659,\n",
       " 'leaders': 660,\n",
       " 'visit': 661,\n",
       " 'stephen': 662,\n",
       " 'hair': 663,\n",
       " 'huge': 664,\n",
       " 'box': 665,\n",
       " 'gift': 666,\n",
       " 'david': 667,\n",
       " 'union': 668,\n",
       " 'kind': 669,\n",
       " 'kid': 670,\n",
       " 'since': 671,\n",
       " 'moment': 672,\n",
       " 'china': 673,\n",
       " 'chinese': 674,\n",
       " 'birth': 675,\n",
       " 'non': 676,\n",
       " 'cop': 677,\n",
       " 'store': 678,\n",
       " 'lessons': 679,\n",
       " 'late': 680,\n",
       " 'hope': 681,\n",
       " 'accused': 682,\n",
       " 'taylor': 683,\n",
       " 'date': 684,\n",
       " 'career': 685,\n",
       " 'interview': 686,\n",
       " 'himself': 687,\n",
       " 'politics': 688,\n",
       " 'weekend': 689,\n",
       " 'called': 690,\n",
       " 'early': 691,\n",
       " 'victims': 692,\n",
       " 'least': 693,\n",
       " 'bring': 694,\n",
       " 'senator': 695,\n",
       " 'whole': 696,\n",
       " 'tom': 697,\n",
       " 'conversation': 698,\n",
       " 'adorable': 699,\n",
       " 'waiting': 700,\n",
       " 'jimmy': 701,\n",
       " 'break': 702,\n",
       " 'sports': 703,\n",
       " 'syria': 704,\n",
       " 'powerful': 705,\n",
       " 'drunk': 706,\n",
       " 'c': 707,\n",
       " 'point': 708,\n",
       " 'united': 709,\n",
       " 'leader': 710,\n",
       " 'anything': 711,\n",
       " 'become': 712,\n",
       " 'investigation': 713,\n",
       " 'opens': 714,\n",
       " 'learned': 715,\n",
       " 'words': 716,\n",
       " 'millions': 717,\n",
       " 'k': 718,\n",
       " 'die': 719,\n",
       " 'fashion': 720,\n",
       " 'cops': 721,\n",
       " \"they're\": 722,\n",
       " 'reality': 723,\n",
       " 'billion': 724,\n",
       " 'fall': 725,\n",
       " 'key': 726,\n",
       " 'true': 727,\n",
       " 'host': 728,\n",
       " 'returns': 729,\n",
       " 'joe': 730,\n",
       " 'totally': 731,\n",
       " 'syrian': 732,\n",
       " 'killing': 733,\n",
       " 'massive': 734,\n",
       " '40': 735,\n",
       " 'almost': 736,\n",
       " 'turn': 737,\n",
       " 'breaks': 738,\n",
       " 'driving': 739,\n",
       " 'mass': 740,\n",
       " 'global': 741,\n",
       " 'dating': 742,\n",
       " 'far': 743,\n",
       " 'policy': 744,\n",
       " 'schools': 745,\n",
       " 'stand': 746,\n",
       " 'trans': 747,\n",
       " 'dinner': 748,\n",
       " 'oil': 749,\n",
       " 'apple': 750,\n",
       " 'un': 751,\n",
       " 'awards': 752,\n",
       " 'queer': 753,\n",
       " 'worried': 754,\n",
       " 'kills': 755,\n",
       " 'iraq': 756,\n",
       " 'low': 757,\n",
       " 'song': 758,\n",
       " 'dance': 759,\n",
       " 'turns': 760,\n",
       " 'puts': 761,\n",
       " 'spends': 762,\n",
       " 'stage': 763,\n",
       " 'sign': 764,\n",
       " 'candidates': 765,\n",
       " 'j': 766,\n",
       " 'vows': 767,\n",
       " 'risk': 768,\n",
       " 'bus': 769,\n",
       " 'names': 770,\n",
       " 'final': 771,\n",
       " 'planned': 772,\n",
       " 'feels': 773,\n",
       " 'anniversary': 774,\n",
       " 'lgbt': 775,\n",
       " 'signs': 776,\n",
       " 'jr': 777,\n",
       " 'murder': 778,\n",
       " 'seen': 779,\n",
       " 'prince': 780,\n",
       " 'reportedly': 781,\n",
       " 'hits': 782,\n",
       " 'light': 783,\n",
       " 'sick': 784,\n",
       " 'adds': 785,\n",
       " 'crash': 786,\n",
       " 'd': 787,\n",
       " 'worst': 788,\n",
       " 'surprise': 789,\n",
       " 'hands': 790,\n",
       " 'near': 791,\n",
       " 'transgender': 792,\n",
       " 'weird': 793,\n",
       " 'nfl': 794,\n",
       " 'return': 795,\n",
       " 'moving': 796,\n",
       " \"there's\": 797,\n",
       " 'pence': 798,\n",
       " 'mind': 799,\n",
       " 'center': 800,\n",
       " 'decision': 801,\n",
       " 'longer': 802,\n",
       " 'workers': 803,\n",
       " 'advice': 804,\n",
       " 'worth': 805,\n",
       " 'eat': 806,\n",
       " 'struggling': 807,\n",
       " 'discover': 808,\n",
       " 'oscar': 809,\n",
       " 'across': 810,\n",
       " 'style': 811,\n",
       " 'kardashian': 812,\n",
       " 'employees': 813,\n",
       " 'test': 814,\n",
       " '13': 815,\n",
       " 'cut': 816,\n",
       " 'keeps': 817,\n",
       " 'band': 818,\n",
       " 'industry': 819,\n",
       " 'experience': 820,\n",
       " 'side': 821,\n",
       " 'coffee': 822,\n",
       " 'check': 823,\n",
       " '2014': 824,\n",
       " 'number': 825,\n",
       " 'rubio': 826,\n",
       " 'brings': 827,\n",
       " 'door': 828,\n",
       " 'lead': 829,\n",
       " 'five': 830,\n",
       " 'completely': 831,\n",
       " 'hoping': 832,\n",
       " 'hand': 833,\n",
       " 'university': 834,\n",
       " '2017': 835,\n",
       " 'official': 836,\n",
       " 'starting': 837,\n",
       " 'lose': 838,\n",
       " 'whether': 839,\n",
       " 'force': 840,\n",
       " 'paris': 841,\n",
       " 'weight': 842,\n",
       " 'road': 843,\n",
       " 'space': 844,\n",
       " 'west': 845,\n",
       " 'audience': 846,\n",
       " 'important': 847,\n",
       " 'steve': 848,\n",
       " 'playing': 849,\n",
       " 'reform': 850,\n",
       " 'cool': 851,\n",
       " 'fighting': 852,\n",
       " 'suspect': 853,\n",
       " 'given': 854,\n",
       " 'defense': 855,\n",
       " 'program': 856,\n",
       " 'artist': 857,\n",
       " 'nyc': 858,\n",
       " 'williams': 859,\n",
       " 'role': 860,\n",
       " 'building': 861,\n",
       " 'michelle': 862,\n",
       " 'peace': 863,\n",
       " 'carolina': 864,\n",
       " 'remember': 865,\n",
       " 'chicago': 866,\n",
       " 'act': 867,\n",
       " 'pro': 868,\n",
       " 'possible': 869,\n",
       " 'apartment': 870,\n",
       " 'governor': 871,\n",
       " 'iowa': 872,\n",
       " 'executive': 873,\n",
       " 'success': 874,\n",
       " 'data': 875,\n",
       " 'chance': 876,\n",
       " 'ferguson': 877,\n",
       " 'amazon': 878,\n",
       " 'biggest': 879,\n",
       " 'protesters': 880,\n",
       " 'suicide': 881,\n",
       " 'hall': 882,\n",
       " 'abuse': 883,\n",
       " 'which': 884,\n",
       " 'clearly': 885,\n",
       " 'major': 886,\n",
       " 'push': 887,\n",
       " 'hurricane': 888,\n",
       " 'moore': 889,\n",
       " 'allegations': 890,\n",
       " 'halloween': 891,\n",
       " 'oscars': 892,\n",
       " 'homeless': 893,\n",
       " 'israel': 894,\n",
       " 'general': 895,\n",
       " 'mental': 896,\n",
       " 'coworker': 897,\n",
       " 'moms': 898,\n",
       " 'board': 899,\n",
       " 'close': 900,\n",
       " 'magazine': 901,\n",
       " 'question': 902,\n",
       " 'ben': 903,\n",
       " 'hear': 904,\n",
       " 'demands': 905,\n",
       " 'fear': 906,\n",
       " 'wishes': 907,\n",
       " 'opening': 908,\n",
       " 'members': 909,\n",
       " 'celebrate': 910,\n",
       " 'supporters': 911,\n",
       " 'google': 912,\n",
       " 'football': 913,\n",
       " 'voice': 914,\n",
       " 'easy': 915,\n",
       " 'teens': 916,\n",
       " 'card': 917,\n",
       " 'kerry': 918,\n",
       " 'wait': 919,\n",
       " 'try': 920,\n",
       " 'throws': 921,\n",
       " 'tour': 922,\n",
       " 'pregnant': 923,\n",
       " 'pizza': 924,\n",
       " 'dying': 925,\n",
       " 'press': 926,\n",
       " 'chicken': 927,\n",
       " 'urges': 928,\n",
       " 'reveal': 929,\n",
       " 'simple': 930,\n",
       " 'green': 931,\n",
       " 'economy': 932,\n",
       " 'problems': 933,\n",
       " 'culture': 934,\n",
       " 'lgbtq': 935,\n",
       " 'asking': 936,\n",
       " 'ebola': 937,\n",
       " 'robert': 938,\n",
       " 'learn': 939,\n",
       " 'performance': 940,\n",
       " 'album': 941,\n",
       " 'church': 942,\n",
       " 'begins': 943,\n",
       " 'officer': 944,\n",
       " 'shop': 945,\n",
       " 'poor': 946,\n",
       " 'uses': 947,\n",
       " 'plane': 948,\n",
       " 'families': 949,\n",
       " 'harassment': 950,\n",
       " 'picture': 951,\n",
       " 'jobs': 952,\n",
       " 'fails': 953,\n",
       " 'sean': 954,\n",
       " 'voter': 955,\n",
       " 'beauty': 956,\n",
       " 'demand': 957,\n",
       " 'doctor': 958,\n",
       " \"we're\": 959,\n",
       " 'spot': 960,\n",
       " 'shares': 961,\n",
       " 'leads': 962,\n",
       " 'hilarious': 963,\n",
       " 'suggests': 964,\n",
       " 'rally': 965,\n",
       " 'results': 966,\n",
       " 'ideas': 967,\n",
       " '18': 968,\n",
       " 'jenner': 969,\n",
       " 'arrested': 970,\n",
       " 'male': 971,\n",
       " 'fuck': 972,\n",
       " 'leaving': 973,\n",
       " 'address': 974,\n",
       " 'rest': 975,\n",
       " 'receives': 976,\n",
       " 'amid': 977,\n",
       " 'epa': 978,\n",
       " 'deadly': 979,\n",
       " 'netflix': 980,\n",
       " 'desperate': 981,\n",
       " 'planet': 982,\n",
       " 'cnn': 983,\n",
       " 'marijuana': 984,\n",
       " 'quietly': 985,\n",
       " 'action': 986,\n",
       " 'website': 987,\n",
       " 'pick': 988,\n",
       " 'explains': 989,\n",
       " 'table': 990,\n",
       " 'energy': 991,\n",
       " 'users': 992,\n",
       " 'feeling': 993,\n",
       " 'sales': 994,\n",
       " 'colbert': 995,\n",
       " 'apparently': 996,\n",
       " \"let's\": 997,\n",
       " 'amazing': 998,\n",
       " 'went': 999,\n",
       " 'budget': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is no word for 0th index\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71263c",
   "metadata": {},
   "source": [
    "#### 6. Create features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b1fa6",
   "metadata": {},
   "source": [
    "#### Define X and y for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a088ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples :  26709\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  307  678 3336 2297   47  381 2575    5\n",
      " 2576 8433]\n",
      "Number of Labels :  26709\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(data['headline'])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "y = np.asarray(data['is_sarcastic'])\n",
    "\n",
    "print(\"Number of Samples : \", len(X))\n",
    "print(X[0])\n",
    "print(\"Number of Labels : \", len(y))\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5d26a",
   "metadata": {},
   "source": [
    "#### 7. Get Vocabulory size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbeeed87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "# since the 0th index doesn't have a word, add 1 to the length of the vocabulory\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b569bc",
   "metadata": {},
   "source": [
    "#### 8. Create a weight matrix using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8717cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Glove word embeddings\n",
    "glove_file = 'glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b77edf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Extract Glove embedding zip file\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(glove_file, 'r') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7e650",
   "metadata": {},
   "source": [
    "##### Get the Word Embeddings using Embedding file as given below.##### Get the Word Embeddings using Embedding file as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd703827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'glove.6B.200d.txt'\n",
    "\n",
    "embeddings = {}\n",
    "for o in open(EMBEDDING_FILE, encoding='utf8'):\n",
    "    word = o.split(\" \")[0]\n",
    "    # print(word)\n",
    "    embd = o.split(\" \")[1:]\n",
    "    embd = np.asarray(embd, dtype='float32')\n",
    "    # print(embd)\n",
    "    embeddings[word] = embd\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "len(embeddings.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027e5f0",
   "metadata": {},
   "source": [
    "#### 9. Define and compile a Bidirectional LSTM model. Hint: Be analytical and experimental here in trying new approaches to design the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b211221",
   "metadata": {},
   "source": [
    "Used Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, flatten it, then dense and dropout layers as required. -In the end added a final dense layer with sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90570587",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(maxlen,),dtype=tf.int64)\n",
    "embed = Embedding(embedding_matrix.shape[0],output_dim=200,weights=[embedding_matrix],input_length=maxlen, trainable=True)(input_layer)\n",
    "lstm=Bidirectional(LSTM(128))(embed)\n",
    "drop=Dropout(0.3)(lstm)\n",
    "dense =Dense(100,activation='relu')(drop)\n",
    "out=Dense(2,activation='softmax')(dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ba7f0",
   "metadata": {},
   "source": [
    "#### 10. Fit the model and check the validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1abb95",
   "metadata": {},
   "source": [
    "fit the model with a batch size of 100 and validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "976ae83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 254)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 254, 200)          5931400   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              336896    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               25700     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,294,198\n",
      "Trainable params: 6,294,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 5\n",
    "\n",
    "model = Model(input_layer,out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e655288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/214 [==============================] - 718s 3s/step - loss: 0.4360 - accuracy: 0.7893\n",
      "Epoch 2/5\n",
      "214/214 [==============================] - 749s 4s/step - loss: 0.2542 - accuracy: 0.8936\n",
      "Epoch 3/5\n",
      "214/214 [==============================] - 751s 4s/step - loss: 0.1651 - accuracy: 0.9360\n",
      "Epoch 4/5\n",
      "214/214 [==============================] - 755s 4s/step - loss: 0.1083 - accuracy: 0.9605\n",
      "Epoch 5/5\n",
      "214/214 [==============================] - 762s 4s/step - loss: 0.0653 - accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aea254fe50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d98b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 14s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(np.array(X_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9ca40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = [1 if j>i else 0 for i,j in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64ae8a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2591,  440],\n",
       "       [ 288, 2023]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b93c3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88      3031\n",
      "           1       0.82      0.88      0.85      2311\n",
      "\n",
      "    accuracy                           0.86      5342\n",
      "   macro avg       0.86      0.87      0.86      5342\n",
      "weighted avg       0.87      0.86      0.86      5342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9132c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
